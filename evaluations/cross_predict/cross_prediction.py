import os
import sys
import numpy as np
import pandas as pd
import joblib
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor
import time

# ================== Configs ==================
# Species classification
# There are some datasets split from the original benchmark datasets of the same name, so a â€˜2â€™ is added to distinguish them.
SPECIES_CATEGORIES = {
    'Plant': {
        'color': 'green',
        'datasets': ['4mC_A.thaliana2', '4mC_C.equisetifolia', '4mC_F.vesca', '4mC_R.chinensis']
    },
    'Animal': {
        'color': 'blue',
        'datasets': ['4mC_C.elegans2', '4mC_D.melanogaster2']
    },
    'Microbe': {
        'color': 'red',
        'datasets': ['4mC_E.coli2', '4mC_G.subterraneus2', 
                    '4mC_G.pickeringii2', '4mC_S.cerevisiae', '4mC_Tolypocladium']
    }
}

CATEGORY_ORDER = ['Plant', 'Animal', 'Microbe']
ORDERED_DATASETS = []
for category in CATEGORY_ORDER:
    ORDERED_DATASETS.extend(SPECIES_CATEGORIES[category]['datasets'])

# ================== Original configs ==================
BASE_MODELS = ['CNN', 'BLSTM', 'Transformer']
PROJECT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(PROJECT_DIR, 'data/4mC')
MODEL_DIR = os.path.join(PROJECT_DIR, 'pretrained_models/5cv')
OUTPUT_DIR = os.path.join(PROJECT_DIR, 'evaluations/cross_predict_12')
os.makedirs(OUTPUT_DIR, exist_ok=True)

sys.path.append(PROJECT_DIR)
from prepare.prepare_ml import ml_code, read_fasta_data
from feature_engineering.feature_selection_eukaryotes import load_top_features, get_feature_methods

# ================== Universal function ==================
def load_best_n(model_name, dataset):
    """Load the optimal number of features"""
    print(f"ğŸ”„ åŠ è½½æœ€ä¼˜ç‰¹å¾æ•°é‡: {model_name} - {dataset}")
    acc_table_path = os.path.join(PROJECT_DIR, "feature_engineering/ifs_result_cross_species", 
                                 f"{model_name}_Feature_Acc_Table.csv")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(acc_table_path):
        print(f"âŒ ç‰¹å¾è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {acc_table_path}")
        return None
    
    df = pd.read_csv(acc_table_path)
    
    # æ£€æŸ¥æ•°æ®é›†åˆ—æ˜¯å¦å­˜åœ¨
    if dataset not in df.columns:
        print(f"âŒ æ•°æ®é›† '{dataset}' ä¸åœ¨ç‰¹å¾è¡¨ä¸­")
        return None
    
    best_n = df[df['N'] == 'best_n'][dataset].values[0]
    return int(float(best_n))

def prepare_source_data(source_dataset, model_type):
    """Prepare training data for the source species"""
    print(f"ğŸ”„ å‡†å¤‡æºæ•°æ®: {source_dataset} ({model_type})")
    
    # Load training data
    train_pos = os.path.join(DATA_DIR, source_dataset, "train_pos.txt")
    train_neg = os.path.join(DATA_DIR, source_dataset, "train_neg.txt")
    
    # æ·»åŠ æ–‡ä»¶æ£€æŸ¥
    if not os.path.exists(train_pos):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {train_pos}")
        return None, None, None
    if not os.path.exists(train_neg):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {train_neg}")
        return None, None, None
    
    # è¯»å–æ•°æ®
    try:
        pos_data = read_fasta_data(train_pos)
        neg_data = read_fasta_data(train_neg)
    except Exception as e:
        print(f"âŒ è¯»å–æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return None, None, None
    
    # create dataFrame
    df = pd.DataFrame({
        "label": [1]*len(pos_data) + [0]*len(neg_data),
        "seq": pos_data + neg_data
    })
    
    # Obtain feature configuration
    best_n = load_best_n(model_type, source_dataset)
    if best_n is None:
        print(f"âŒ æ— æ³•è·å–æœ€ä¼˜ç‰¹å¾æ•°é‡")
        return None, None, None
    
    features = load_top_features(model_type, source_dataset, best_n)
    feature_methods = get_feature_methods(features)
    
    # generate features
    try:
        X_train, y_train, _ = ml_code(df, "training", feature_methods)
    except Exception as e:
        print(f"âŒ ç”Ÿæˆç‰¹å¾æ—¶å‡ºé”™: {str(e)}")
        return None, None, None
    
    # æ·»åŠ å®Œæˆæç¤º
    print(f"âœ… æºæ•°æ®å‡†å¤‡å®Œæˆ: {source_dataset} ({model_type}) - æ ·æœ¬æ•°: {len(X_train)}")
    return X_train, y_train, feature_methods

def prepare_target_data(target_dataset, feature_methods):
    """Prepare target test data"""
    print(f"ğŸ”„ å‡†å¤‡ç›®æ ‡æ•°æ®: {target_dataset}")
    
    test_pos = os.path.join(DATA_DIR, target_dataset, "test_pos.txt")
    test_neg = os.path.join(DATA_DIR, target_dataset, "test_neg.txt")
    
    # æ·»åŠ æ–‡ä»¶æ£€æŸ¥
    if not os.path.exists(test_pos):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {test_pos}")
        return None, None
    if not os.path.exists(test_neg):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {test_neg}")
        return None, None
    
    # è¯»å–æ•°æ®
    try:
        pos_data = read_fasta_data(test_pos)
        neg_data = read_fasta_data(test_neg)
    except Exception as e:
        print(f"âŒ è¯»å–æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return None, None
    
    df = pd.DataFrame({
        "label": [1]*len(pos_data) + [0]*len(neg_data),
        "seq": pos_data + neg_data
    })
    
    try:
        X_test, y_test, _ = ml_code(df, "testing", feature_methods)
    except Exception as e:
        print(f"âŒ ç”Ÿæˆç‰¹å¾æ—¶å‡ºé”™: {str(e)}")
        return None, None
    
    # æ·»åŠ å®Œæˆæç¤º
    print(f"âœ… ç›®æ ‡æ•°æ®å‡†å¤‡å®Œæˆ: {target_dataset} - æ ·æœ¬æ•°: {len(X_test)}")
    return X_test, y_test

def predict_single_case(source, target):
    """Perform single cross prediction"""
    try:
        start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
        
        # æ·»åŠ è¿›åº¦æç¤º
        print(f"\nğŸ” å¼€å§‹å¤„ç†: {source} -> {target}")
        
        # Load ensemble model
        ensemble_path = os.path.join(MODEL_DIR, f'ensemble_5cv_{source}.pkl')
        print(f"ğŸ”„ åŠ è½½é›†æˆæ¨¡å‹: {ensemble_path}")
        
        # æ£€æŸ¥é›†æˆæ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(ensemble_path):
            print(f"âŒ é›†æˆæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {ensemble_path}")
            return np.nan, np.nan
        
        try:
            ensemble_model = joblib.load(ensemble_path)
        except Exception as e:
            print(f"âŒ åŠ è½½é›†æˆæ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
            return np.nan, np.nan
        
        meta_features = []
        y_target = None
        
        for model_type in BASE_MODELS:
            model_start = time.time()  # è®°å½•æ¨¡å‹å¼€å§‹æ—¶é—´
            print(f"  ğŸ”„ å¤„ç†åŸºç¡€æ¨¡å‹: {model_type}")
            
            # å‡†å¤‡æºæ•°æ®
            X_source_train, _, feature_methods = prepare_source_data(source, model_type)
            if X_source_train is None:
                print(f"  âŒ æºæ•°æ®å‡†å¤‡å¤±è´¥")
                return np.nan, np.nan
                
            # å‡†å¤‡ç›®æ ‡æ•°æ®
            X_target, y_target = prepare_target_data(target, feature_methods)
            if X_target is None:
                print(f"  âŒ ç›®æ ‡æ•°æ®å‡†å¤‡å¤±è´¥")
                return np.nan, np.nan
                
            # æ ‡å‡†åŒ–
            print(f"  ğŸ”„ æ ‡å‡†åŒ–æ•°æ®")
            try:
                scaler = StandardScaler().fit(X_source_train)
                X_target_scaled = scaler.transform(X_target)
            except Exception as e:
                print(f"  âŒ æ ‡å‡†åŒ–å¤±è´¥: {str(e)}")
                return np.nan, np.nan
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            model_path = os.path.join(MODEL_DIR, f'{model_type.lower()}_best_{source}.h5')
            print(f"  ğŸ”„ åŠ è½½æ¨¡å‹: {model_path}")
            
            # æ·»åŠ æ¨¡å‹æ–‡ä»¶æ£€æŸ¥
            if not os.path.exists(model_path):
                print(f"  âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                return np.nan, np.nan
                
            try:
                base_model = tf.keras.models.load_model(model_path)
            except Exception as e:
                print(f"  âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
                return np.nan, np.nan
            
            # é¢„æµ‹
            print(f"  ğŸ”„ è¿›è¡Œé¢„æµ‹")
            try:
                input_data = X_target_scaled.reshape(-1, 1, X_target_scaled.shape[1])
                preds = base_model.predict(input_data, verbose=0).flatten()
                meta_features.append(preds)
            except Exception as e:
                print(f"  âŒ é¢„æµ‹å¤±è´¥: {str(e)}")
                return np.nan, np.nan
            
            # æ·»åŠ æ¨¡å‹å¤„ç†å®Œæˆæç¤º
            model_time = time.time() - model_start
            print(f"  âœ… {model_type} æ¨¡å‹å¤„ç†å®Œæˆ - è€—æ—¶: {model_time:.2f}ç§’")
        
        # Composite elemental features
        meta_X = np.column_stack(meta_features)
        
        # Integrated prediction
        print(f"ğŸ”„ è¿›è¡Œé›†æˆé¢„æµ‹")
        try:
            y_pred = ensemble_model.predict(meta_X)
            if hasattr(ensemble_model, 'predict_proba'):
                y_proba = ensemble_model.predict_proba(meta_X)[:, 1]
            else:
                y_proba = y_pred
        except Exception as e:
            print(f"âŒ é›†æˆé¢„æµ‹å¤±è´¥: {str(e)}")
            return np.nan, np.nan
        
        # è®¡ç®—æŒ‡æ ‡
        try:
            acc = accuracy_score(y_target, y_pred)
            auc = roc_auc_score(y_target, y_proba)
        except Exception as e:
            print(f"âŒ è®¡ç®—æŒ‡æ ‡å¤±è´¥: {str(e)}")
            return np.nan, np.nan
        
        # æ·»åŠ å®Œæˆæç¤º
        total_time = time.time() - start_time
        print(f"âœ… å®Œæˆå¤„ç†: {source} -> {target} - å‡†ç¡®ç‡: {acc:.4f}, AUC: {auc:.4f} - æ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        return acc, auc
    
    except Exception as e:
        print(f"âŒ å¤„ç† {source}->{target} æ—¶å‡ºé”™: {str(e)}")
        return np.nan, np.nan

# ================== Visualization ==================
def generate_heatmap(matrix, metric_name):
    print(f"ğŸ–¼ï¸ ç”Ÿæˆ{metric_name}çƒ­åŠ›å›¾")
    
    plt.figure(figsize=(18, 15))
    ax = sns.heatmap(
        matrix.astype(float), 
        annot=True, 
        fmt=".3f",
        cmap="coolwarm",
        cbar_kws={'label': metric_name},
        vmin=0.5,
        vmax=1.0,
        linewidths=0.5,
        annot_kws={"size": 9}
    )
    
    plt.title(f'Cross-Dataset {metric_name}', fontsize=16, pad=25)
    ax.xaxis.set_label_position('top') 
    ax.xaxis.tick_top()
    plt.xlabel('Test Dataset', fontsize=14, labelpad=15)
    plt.ylabel('Train Dataset', fontsize=14, labelpad=15)

    ax.set_xticks(np.arange(len(ORDERED_DATASETS)) + 0.5)
    ax.set_xticklabels(ORDERED_DATASETS, rotation=45, ha='left', fontsize=10)
    ax.set_yticks(np.arange(len(ORDERED_DATASETS)) + 0.5)
    ax.set_yticklabels(ORDERED_DATASETS, rotation=0, fontsize=10)
    
    # Set classification label color
    def set_label_colors(labels, axis='x'):
        for label in labels:
            text = label.get_text()
            for category, info in SPECIES_CATEGORIES.items():
                if text in info['datasets']:
                    label.set_color(info['color'])
                    label.set_fontweight('bold')
                    if axis == 'x':
                        label.set_rotation(30)
                    break
    
    set_label_colors(ax.get_xticklabels(), 'x')
    set_label_colors(ax.get_yticklabels(), 'y')
    
    def draw_category_lines():
        accum_idx = 0
        for category in CATEGORY_ORDER:
            n = len(SPECIES_CATEGORIES[category]['datasets'])
            accum_idx += n
            ax.axhline(y=accum_idx, color='black', linewidth=2)
            ax.axvline(x=accum_idx, color='black', linewidth=2)
    
    draw_category_lines()
    
    plt.tight_layout()
    
    png_path = os.path.join(OUTPUT_DIR, f'{metric_name.lower()}_heatmap.png')
    csv_path = os.path.join(OUTPUT_DIR, f'{metric_name.lower()}_matrix.csv')
    
    plt.savefig(png_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    # Save sorted CSV
    ordered_matrix = matrix.reindex(index=ORDERED_DATASETS, columns=ORDERED_DATASETS)
    ordered_matrix.to_csv(csv_path, float_format='%.4f')
    
    print(f"âœ… {metric_name}çƒ­åŠ›å›¾å·²ä¿å­˜: {png_path}")
    print(f"âœ… {metric_name}çŸ©é˜µå·²ä¿å­˜: {csv_path}")

# ================== Main ==================
def main():
    # æ·»åŠ æ€»ä½“å¼€å§‹æç¤º
    print(f"ğŸš€ å¼€å§‹è·¨æ•°æ®é›†é¢„æµ‹ä»»åŠ¡")
    print(f"æ•°æ®é›†æ•°é‡: {len(ORDERED_DATASETS)}")
    print(f"æ€»ä»»åŠ¡æ•°: {len(ORDERED_DATASETS)**2}")
    print(f"ä½¿ç”¨çº¿ç¨‹æ•°: {os.cpu_count()}")
    print(f"é¡¹ç›®ç›®å½•: {PROJECT_DIR}")
    print(f"æ•°æ®ç›®å½•: {DATA_DIR}")
    print(f"æ¨¡å‹ç›®å½•: {MODEL_DIR}")
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    
    # Initialize result matrix (using ordered species list)
    acc_matrix = pd.DataFrame(index=ORDERED_DATASETS, columns=ORDERED_DATASETS, dtype=float)
    auc_matrix = pd.DataFrame(index=ORDERED_DATASETS, columns=ORDERED_DATASETS, dtype=float)

    # parallel processing
    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:
        futures = []
        for source in ORDERED_DATASETS:
            for target in ORDERED_DATASETS:
                futures.append(executor.submit(predict_single_case, source, target))

        # Fill in results
        progress = tqdm(total=len(futures), desc="å¤„ç†è·¨æ•°æ®é›†é¢„æµ‹")
        for i, future in enumerate(futures):
            source_idx = i // len(ORDERED_DATASETS)
            target_idx = i % len(ORDERED_DATASETS)
            source = ORDERED_DATASETS[source_idx]
            target = ORDERED_DATASETS[target_idx]
            
            acc, auc = future.result()
            acc_matrix.loc[source, target] = acc
            auc_matrix.loc[source, target] = auc
            progress.update()
        progress.close()

    # æ·»åŠ å¯è§†åŒ–æç¤º
    print(f"ğŸ–¼ï¸ ç”Ÿæˆå‡†ç¡®ç‡çƒ­åŠ›å›¾")
    generate_heatmap(acc_matrix, 'Accuracy')
    
    print(f"ğŸ–¼ï¸ ç”ŸæˆAUCçƒ­åŠ›å›¾")
    generate_heatmap(auc_matrix, 'AUC')
    
    # æ·»åŠ æ€»ä½“å®Œæˆæç¤º
    print(f"ğŸ‰ ä»»åŠ¡å®Œæˆ! ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_DIR}")
    
    # è®¡ç®—å¹¶æ˜¾ç¤ºNaNå€¼çš„æ•°é‡
    nan_count_acc = acc_matrix.isna().sum().sum()
    nan_count_auc = auc_matrix.isna().sum().sum()
    print(f"å‡†ç¡®ç‡çŸ©é˜µä¸­ç¼ºå¤±å€¼æ•°é‡: {nan_count_acc}")
    print(f"AUCçŸ©é˜µä¸­ç¼ºå¤±å€¼æ•°é‡: {nan_count_auc}")

if __name__ == "__main__":
    main()